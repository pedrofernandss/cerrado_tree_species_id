{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8510679,"sourceType":"datasetVersion","datasetId":5080300},{"sourceId":8516220,"sourceType":"datasetVersion","datasetId":5084442},{"sourceId":9328743,"sourceType":"datasetVersion","datasetId":5651986},{"sourceId":10689864,"sourceType":"datasetVersion","datasetId":6623428},{"sourceId":10689982,"sourceType":"datasetVersion","datasetId":6623527},{"sourceId":10692058,"sourceType":"datasetVersion","datasetId":6624935}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!sudo apt install libimage-exiftool-perl -y","metadata":{"id":"Djxv8BZlcZgv","outputId":"d4bdebb0-f402-4237-e134-343bd1454e9f","execution":{"iopub.status.busy":"2024-05-27T12:59:14.354175Z","iopub.execute_input":"2024-05-27T12:59:14.354637Z","iopub.status.idle":"2024-05-27T12:59:18.218434Z","shell.execute_reply.started":"2024-05-27T12:59:14.354603Z","shell.execute_reply":"2024-05-27T12:59:18.216678Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import subprocess\nfrom PIL import Image\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport glob\nimport os\n\n\"\"\"\n    get XML metadata\n\"\"\"\n\ndef get_xml_metadata(imgPath):\n  infoDict = {}\n  exifToolPath = 'exiftool'\n  ''' use Exif tool to get the metadata '''\n  process = subprocess.Popen([exifToolPath,imgPath],stdout=subprocess.PIPE, stderr=subprocess.STDOUT,universal_newlines=True)\n  ''' get the tags in dict '''\n  for tag in process.stdout:\n      line = tag.strip().split(':')\n      infoDict[line[0].strip()] = line[-1].strip()\n  return infoDict\n\n\"\"\"\n    Vignette Correction\n\"\"\"\n\ndef vig_correct(image_path, infoDict):\n  centerX = int(float(infoDict['Calibrated Optical Center X']))\n  centerY = int(float(infoDict['Calibrated Optical Center Y']))\n  k = [float(elem.strip(\",\")) for elem in infoDict['Vignetting Data'].split()]\n\n  # Load the image\n  image = Image.open(image_path)\n  np_img = np.array(image, dtype=np.uint16)\n  rows, cols = np_img.shape[:2]\n\n  # Create meshgrid of coordinates\n  y_coords, x_coords = np.meshgrid(np.arange(rows), np.arange(cols), indexing='ij')\n\n  # Calculate the distance r for each pixel\n  r = np.sqrt((x_coords - centerX)**2 + (y_coords - centerY)**2)\n\n  # Calculate the correction factor\n  correction_factor = (\n      k[5] * r**6 +\n      k[4] * r**5 +\n      k[3] * r**4 +\n      k[2] * r**3 +\n      k[1] * r**2 +\n      k[0] * r +\n      1.0\n  )\n\n  # Apply the correction factor to the image\n  corrected_img = np_img * correction_factor[..., np.newaxis] if np_img.ndim == 3 else np_img * correction_factor\n  return corrected_img.astype('uint16')\n\n\"\"\"\n    Distortion correction\n\"\"\"\ndef undistort(new_img, infoDict):\n  centerX = int(float(infoDict['Calibrated Optical Center X']))\n  centerY = int(float(infoDict['Calibrated Optical Center Y']))\n\n  dewarp_args = [float(elem) for elem in infoDict['Dewarp Data'].split(\";\")[1].split(\",\")]\n  dist_coeffs = np.asarray(dewarp_args[4:])\n  tuple0 = (dewarp_args[0], 0, centerX + dewarp_args[2])\n  tuple1 = (0, dewarp_args[1], centerY + dewarp_args[3])\n  tuple2 = (0, 0, 1)\n  camera_matrix = np.asarray([tuple0, tuple1, tuple2])\n\n  h,  w = new_img.shape[:2]\n  _, roi=cv2.getOptimalNewCameraMatrix(camera_matrix,dist_coeffs,(w,h),1,(w,h))\n\n  dst = cv2.undistort(new_img, camera_matrix, dist_coeffs, None, camera_matrix)\n  # crop the image\n  x,y,w,h = roi\n  dst = dst[y:y+h, x:x+w]\n  return dst\n\n\n\"\"\"\n    Alignment of the phase and rotation differences caused by different camera locations and\n    optical accuracy\n\"\"\"\ndef align_phase_rotation(new_img, infoDict):\n  rows, cols = new_img.shape\n  H = np.asarray([float(elem) for elem in infoDict['Calibrated H Matrix'].split(\",\")]).reshape(3,3)\n  new_img = cv2.warpPerspective(new_img, H, (cols, rows))\n  return new_img\n","metadata":{"id":"Iry3p3ktYq_K","execution":{"iopub.status.busy":"2024-05-27T12:59:18.221471Z","iopub.execute_input":"2024-05-27T12:59:18.221899Z","iopub.status.idle":"2024-05-27T12:59:18.247982Z","shell.execute_reply.started":"2024-05-27T12:59:18.221861Z","shell.execute_reply":"2024-05-27T12:59:18.246236Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\"\n\nAlignment of the difference caused by different exposure times\n\n1. Smoothing the images using a Gaussian filter\n\n2. Apply an edge detection filter (ex. Sobel filter) to detect edge lines from the\ntwo images that need to be aligned.\n\n3. Apply an alignment algorithm such as the\nEnhanced Correlation Coefficient (ECC) Maximization to the images.\n\n\"\"\"\n\nfrom scipy.ndimage import gaussian_filter\n\ndef smooth_image(image, sigma=1):\n    \"\"\"Apply Gaussian smoothing to the image.\"\"\"\n    return gaussian_filter(image, sigma=sigma)\n\ndef edge_detection(image):\n    \"\"\"Apply Sobel filter to detect edges in the image.\"\"\"\n    grad_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n    grad_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n    magnitude = cv2.magnitude(grad_x, grad_y)\n    return magnitude\n\n\n\ndef crop_center(img, crop_tamanho):\n    # Verificar as dimensÃµes da imagem\n  altura = img.shape[0]\n  largura = img.shape[1]\n\n  # Calcular as coordenadas do crop central\n  start_x = largura // 2 - crop_tamanho // 2\n  start_y = altura // 2 - crop_tamanho // 2\n  end_x = start_x + crop_tamanho\n  end_y = start_y + crop_tamanho\n\n  # Realizar o crop central\n  imagem_cropped = img[start_y:end_y, start_x:end_x]\n  return imagem_cropped\n\ndef zoom_center(img, zoom_factor=1.5):\n\n    y_size = img.shape[0]\n    x_size = img.shape[1]\n\n    # define new boundaries\n    x1 = int(0.5*x_size*(1-1/zoom_factor))\n    x2 = int(x_size-0.5*x_size*(1-1/zoom_factor))\n    y1 = int(0.5*y_size*(1-1/zoom_factor))\n    y2 = int(y_size-0.5*y_size*(1-1/zoom_factor))\n\n    # first crop image then scale\n    img_cropped = img[y1:y2,x1:x2]\n    return cv2.resize(img_cropped, None, fx=zoom_factor, fy=zoom_factor)\n\nIMG_REF_SHAPE = (2570, 1925)\ndef process_image(imgPath):\n  ##### check image 'Image Source': 'MS_G_CAMERA' for camera type\n\n  # get xml metadata for camera corrections\n  infoDict = get_xml_metadata(imgPath)\n\n  # custom pipeline for jpg images, because they have a different resolution \n  if imgPath[-3:] == 'JPG':\n        new_img = cv2.imread(imgPath)\n        new_img = zoom_center(new_img, 1.3)\n        new_img = cv2.resize(new_img, IMG_REF_SHAPE)\n        new_img = crop_center(new_img, 1500)\n        return new_img\n\n\n  # apply vignette correction\n  new_img = vig_correct(imgPath, infoDict)\n\n  # undistort image\n  new_img = undistort(new_img, infoDict)\n\n  # align phase and rotation\n  new_img = align_phase_rotation(new_img, infoDict)\n  \n  # crop center\n  new_img = crop_center(new_img, 1500)\n\n  return new_img","metadata":{"id":"Bz_CgW-myTmS","execution":{"iopub.status.busy":"2024-05-27T12:59:18.249797Z","iopub.execute_input":"2024-05-27T12:59:18.250224Z","iopub.status.idle":"2024-05-27T12:59:18.270340Z","shell.execute_reply.started":"2024-05-27T12:59:18.250165Z","shell.execute_reply":"2024-05-27T12:59:18.268843Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n\n    Apply alignment algorithm!\n\n\"\"\"\ndef align_images_using_ecc(reference_image, target_image, jpg=False):\n    if jpg:\n        # save colored jpg\n        colored_jpg = target_image\n        # create temporary greyscale img\n        target_image = cv2.cvtColor(target_image, cv2.COLOR_BGR2GRAY) \n    \n    # Smooth the images\n    reference_image_smoothed = smooth_image(reference_image)\n    target_image_smoothed = smooth_image(target_image)\n\n    # Apply edge detection\n    base_edges = edge_detection(reference_image_smoothed)\n    target_edges = edge_detection(target_image_smoothed)\n\n    # Convert images to float32 for ECC algorithm\n    base_edges = base_edges.astype(np.float32)\n    target_edges = target_edges.astype(np.float32)\n    \n    # Define the motion model\n    warp_mode = cv2.MOTION_HOMOGRAPHY\n    \n    # Define 3x3 transformation matrix\n    warp_matrix = np.eye(3, 3, dtype=np.float32)\n\n    # Set the number of iterations and termination criteria\n    if jpg:\n        number_of_iterations = 500\n    else:\n        number_of_iterations = 25\n    termination_eps = 1e-10\n\n    criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, number_of_iterations, termination_eps)\n\n\n    # Apply the ECC algorithm to find the warp matrix\n    cc, warp_matrix = cv2.findTransformECC(base_edges, target_edges, warp_matrix, warp_mode, criteria)\n    \n    if jpg:\n        target_image = colored_jpg\n\n    # Warp the target image to align with the base image\n    aligned_image = cv2.warpPerspective(target_image, warp_matrix, (reference_image.shape[1], reference_image.shape[0]), flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP)\n    \n    return aligned_image","metadata":{"id":"Ptm8mwjrudQq","execution":{"iopub.status.busy":"2024-05-27T12:59:18.273155Z","iopub.execute_input":"2024-05-27T12:59:18.273660Z","iopub.status.idle":"2024-05-27T12:59:18.289210Z","shell.execute_reply.started":"2024-05-27T12:59:18.273610Z","shell.execute_reply":"2024-05-27T12:59:18.288157Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DRIVER\nimport math\nfrom itertools import cycle , islice\n\n# Take relative paths, because chenged os current dir\nos.chdir(\"/kaggle/input/27-01-25-voo-3/DJI_202501271001_009_Linear-Flight-Mission2-copy\")\ntypes = ('*.JPG', '*.TIF') # the tuple of file types\nfiles_grabbed = []\nfor files in types:\n    files_grabbed.extend(glob.glob(files))\n    \ninput_imgs = sorted(files_grabbed)[1030:2060]\n\ni = cycle(input_imgs)\nslc = 5\nfor _ in range(math.ceil(len(input_imgs)/slc)):\n    cur_imgs = list(islice(i,slc))\n    \n    try:\n\n        ref_img = process_image(cur_imgs[1]) # G BAND -> REFERENCE\n        target_jpg = process_image(cur_imgs[0])\n        target_nir = process_image(cur_imgs[2])\n        target_r = process_image(cur_imgs[3])\n        target_re = process_image(cur_imgs[4])\n\n        # Align image to G BAND\n        aligned_jpg_image = align_images_using_ecc(ref_img, target_jpg, True)\n        aligned_nir_image = align_images_using_ecc(ref_img, target_nir)\n        aligned_r_image = align_images_using_ecc(ref_img, target_r)\n        aligned_re_image = align_images_using_ecc(ref_img, target_re)\n\n        # Remember to crop and normalize reference/target img here:\n        # Normalize the image to range 0 to 255 and convert to uint8\n        aligned_jpg_image = cv2.normalize(aligned_jpg_image, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n        aligned_nir_image = cv2.normalize(aligned_nir_image, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n        aligned_r_image = cv2.normalize(aligned_r_image, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n        aligned_re_image = cv2.normalize(aligned_re_image, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n        aligned_g_image = cv2.normalize(ref_img, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n\n        # crop again\n        aligned_jpg_image = crop_center(aligned_jpg_image, 1000)\n        aligned_nir_image = crop_center(aligned_nir_image, 1000)\n        aligned_r_image = crop_center(aligned_r_image, 1000)\n        aligned_re_image = crop_center(aligned_re_image, 1000)\n        aligned_g_image = crop_center(aligned_g_image, 1000)\n\n        # save\n        cv2.imwrite(f\"/kaggle/working/processed-{cur_imgs[0]}\", aligned_jpg_image)\n        cv2.imwrite(f\"/kaggle/working/processed-{cur_imgs[1]}\", aligned_g_image)\n        cv2.imwrite(f\"/kaggle/working/processed-{cur_imgs[2]}\", aligned_nir_image)\n        cv2.imwrite(f\"/kaggle/working/processed-{cur_imgs[3]}\", aligned_r_image)\n        cv2.imwrite(f\"/kaggle/working/processed-{cur_imgs[4]}\", aligned_re_image)\n    except:\n        print(f\"erro no batch: {cur_imgs}\")\n","metadata":{"id":"GJKIZt-7v-n1","outputId":"6cd1b2ad-0890-41a7-eaa7-298a553072fa","execution":{"iopub.status.busy":"2024-05-27T12:59:18.291057Z","iopub.execute_input":"2024-05-27T12:59:18.292799Z","iopub.status.idle":"2024-05-27T13:02:32.793050Z","shell.execute_reply.started":"2024-05-27T12:59:18.292683Z","shell.execute_reply":"2024-05-27T13:02:32.791341Z"},"trusted":true},"outputs":[],"execution_count":null}]}